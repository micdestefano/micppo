# micppo: Another Proximal Policy Optimization (PPO) implementation
This is my implementation of Proximal Policy Optimization.

I wrote this code following [a tutorial from Costa Huang](https://www.youtube.com/watch?v=MEt6rrxH8W4).

## Author
Michele De Stefano
